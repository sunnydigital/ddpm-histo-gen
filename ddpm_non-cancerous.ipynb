{"cells":[{"cell_type":"code","execution_count":null,"id":"MlktmBp9bTRq","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23128,"status":"ok","timestamp":1684134021291,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"MlktmBp9bTRq","outputId":"bcde5e65-d96e-4394-d1d1-0accd2860309"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q -U einops datasets matplotlib tqdm pytz icecream\n","\n","import math\n","\n","import copy\n","import os\n","\n","import csv\n","\n","from pathlib import Path\n","from inspect import isfunction\n","from functools import partial\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from plotly.subplots import make_subplots\n","import plotly.graph_objs as go\n","\n","from tqdm.auto import tqdm\n","\n","from datetime import datetime\n","from pytz import timezone\n","\n","from einops import rearrange\n","\n","from scipy.stats import entropy\n","from scipy.linalg import sqrtm\n","\n","from PIL import Image, ImageDraw\n","\n","from IPython.display import HTML\n","\n","import torch\n","from torch import nn, einsum\n","import torch.nn.functional as F\n","from torch.utils.data import random_split, Dataset, DataLoader, Subset\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","import cv2\n","from skimage.metrics import structural_similarity as ssim\n","\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from torchvision import utils\n","from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize\n","from torchvision.utils import save_image\n","\n","from google.colab import files\n","from google.cloud import storage\n","\n","from datasets import load_dataset\n","\n","import pickle\n","\n","import json\n","\n","import glob\n","\n","from icecream import ic\n","\n","import re\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"id":"K10ye-uahWT4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23193,"status":"ok","timestamp":1684134044480,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"K10ye-uahWT4","outputId":"638d9645-b65d-4f00-9c54-bc72059236f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"nDNGS2yS9eDS","metadata":{"id":"nDNGS2yS9eDS"},"outputs":[],"source":["DOWNLOAD_DATA = False"]},{"cell_type":"code","execution_count":null,"id":"Hz3p4dznbdY9","metadata":{"id":"Hz3p4dznbdY9"},"outputs":[],"source":["if DOWNLOAD_DATA:\n","    %cp kaggle.json /content/drive/MyDrive/kaggle\n","    ! mkdir ~/.kaggle\n","    %cp /content/drive/MyDrive/kaggle/kaggle.json ~/.kaggle/\n","    ! chmod 600 ~/.kaggle/kaggle.json\n","    %cd /content/drive/MyDrive/cv-final-project/data\n","    ! kaggle competitions download -c histopathologic-cancer-detection --force\n","    %cd /content"]},{"cell_type":"code","execution_count":null,"id":"RQj6TJLkbfEU","metadata":{"id":"RQj6TJLkbfEU"},"outputs":[],"source":["! unzip /content/drive/MyDrive/cv-final-project/data/histopathologic-cancer-detection.zip > /dev/null"]},{"cell_type":"code","execution_count":null,"id":"Djh-3NQ-bghC","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1684134192200,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"Djh-3NQ-bghC","outputId":"f1115664-2055-488b-9a20-edd86416783b"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-c1a5c3ae-7b73-4d3c-8170-ebe43f8a94a9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>f38a6374c348f90b587e046aac6079959adf3835</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>c18f2d887b7ae4f6742ee445113fa1aef383ed77</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>755db6279dae599ebb4d39a9123cce439965282d</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>bc3f0c64fb968ff4a8bd33af6971ecae77c75e08</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>068aba587a4950175d04c680d38943fd488d6a9d</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>acfe80838488fae3c89bd21ade75be5c34e66be7</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>a24ce148f6ffa7ef8eefb4efb12ebffe8dd700da</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7f6ccae485af121e0b6ee733022e226ee6b0c65f</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>559e55a64c9ba828f700e948f6886f4cea919261</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>8eaaa7a400aa79d36c2440a4aa101cc14256cda4</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c1a5c3ae-7b73-4d3c-8170-ebe43f8a94a9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c1a5c3ae-7b73-4d3c-8170-ebe43f8a94a9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c1a5c3ae-7b73-4d3c-8170-ebe43f8a94a9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                         id  label\n","0  f38a6374c348f90b587e046aac6079959adf3835      0\n","1  c18f2d887b7ae4f6742ee445113fa1aef383ed77      1\n","2  755db6279dae599ebb4d39a9123cce439965282d      0\n","3  bc3f0c64fb968ff4a8bd33af6971ecae77c75e08      0\n","4  068aba587a4950175d04c680d38943fd488d6a9d      0\n","5  acfe80838488fae3c89bd21ade75be5c34e66be7      0\n","6  a24ce148f6ffa7ef8eefb4efb12ebffe8dd700da      1\n","7  7f6ccae485af121e0b6ee733022e226ee6b0c65f      1\n","8  559e55a64c9ba828f700e948f6886f4cea919261      0\n","9  8eaaa7a400aa79d36c2440a4aa101cc14256cda4      0"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["labels_df = pd.read_csv('train_labels.csv')\n","labels_df.head(10)"]},{"cell_type":"code","execution_count":null,"id":"_T1vZUcdbriS","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1684134192201,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"_T1vZUcdbriS","outputId":"75f2ad84-4538-40b0-d0bd-89eb9f58807b"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-e0f0c14b-d67c-4811-8545-3bf3fe8365b0\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>c18f2d887b7ae4f6742ee445113fa1aef383ed77</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>a24ce148f6ffa7ef8eefb4efb12ebffe8dd700da</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7f6ccae485af121e0b6ee733022e226ee6b0c65f</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>c3d660212bf2a11c994e0eadff13770a9927b731</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>94fa32b29cc1c00403176c0795fffa3cfaa0f20e</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>0b820b71670c039dd0a51333d1c919f471a9e940</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>d34af1e7500f2f3de41b0e6fdeb2ed245d814590</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>464327050ef07bb927f8bfb5c4e4dd5ebd4d3c09</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>6961bdcc16f6c1d7db88fc6a7823178288c2a29e</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>233bf46a575c1731821073e318c029e5df8b12ff</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e0f0c14b-d67c-4811-8545-3bf3fe8365b0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e0f0c14b-d67c-4811-8545-3bf3fe8365b0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e0f0c14b-d67c-4811-8545-3bf3fe8365b0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                          id  label\n","1   c18f2d887b7ae4f6742ee445113fa1aef383ed77      1\n","6   a24ce148f6ffa7ef8eefb4efb12ebffe8dd700da      1\n","7   7f6ccae485af121e0b6ee733022e226ee6b0c65f      1\n","11  c3d660212bf2a11c994e0eadff13770a9927b731      1\n","14  94fa32b29cc1c00403176c0795fffa3cfaa0f20e      1\n","17  0b820b71670c039dd0a51333d1c919f471a9e940      1\n","19  d34af1e7500f2f3de41b0e6fdeb2ed245d814590      1\n","23  464327050ef07bb927f8bfb5c4e4dd5ebd4d3c09      1\n","24  6961bdcc16f6c1d7db88fc6a7823178288c2a29e      1\n","28  233bf46a575c1731821073e318c029e5df8b12ff      1"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["malignant_labels_df = labels_df.loc[labels_df['label'] == 1]\n","malignant_labels_df.head(10)"]},{"cell_type":"code","execution_count":null,"id":"8xTj5iiUbr-g","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1684134192201,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"8xTj5iiUbr-g","outputId":"6a399061-85e6-48ae-c6d5-5dca55e26e68"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-4357c222-aa66-49a1-b385-5c09e70206c0\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>f38a6374c348f90b587e046aac6079959adf3835</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>755db6279dae599ebb4d39a9123cce439965282d</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>bc3f0c64fb968ff4a8bd33af6971ecae77c75e08</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>068aba587a4950175d04c680d38943fd488d6a9d</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>acfe80838488fae3c89bd21ade75be5c34e66be7</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>559e55a64c9ba828f700e948f6886f4cea919261</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>8eaaa7a400aa79d36c2440a4aa101cc14256cda4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>a106469bbfda4cdc5a9da7ac0152927bf1b4a92d</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>a1991e73a9b676faddd2bd47c39754b14d1eb923</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>08566ce82d4406f464c9c2a3cd014704735db7a9</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4357c222-aa66-49a1-b385-5c09e70206c0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4357c222-aa66-49a1-b385-5c09e70206c0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4357c222-aa66-49a1-b385-5c09e70206c0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                          id  label\n","0   f38a6374c348f90b587e046aac6079959adf3835      0\n","2   755db6279dae599ebb4d39a9123cce439965282d      0\n","3   bc3f0c64fb968ff4a8bd33af6971ecae77c75e08      0\n","4   068aba587a4950175d04c680d38943fd488d6a9d      0\n","5   acfe80838488fae3c89bd21ade75be5c34e66be7      0\n","8   559e55a64c9ba828f700e948f6886f4cea919261      0\n","9   8eaaa7a400aa79d36c2440a4aa101cc14256cda4      0\n","10  a106469bbfda4cdc5a9da7ac0152927bf1b4a92d      0\n","12  a1991e73a9b676faddd2bd47c39754b14d1eb923      0\n","13  08566ce82d4406f464c9c2a3cd014704735db7a9      0"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["non_malignant_labels_df = labels_df.loc[labels_df['label'] == 0]\n","non_malignant_labels_df.head(10)"]},{"cell_type":"code","execution_count":null,"id":"0m4kMHkncWSO","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1684134192545,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"0m4kMHkncWSO","outputId":"2cfe114a-c507-42e6-c544-7cfd59e91612"},"outputs":[{"data":{"text/plain":["['.config',\n"," 'drive',\n"," 'train',\n"," 'train_labels.csv',\n"," 'sample_submission.csv',\n"," 'test',\n"," 'sample_data']"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["os.listdir('.')"]},{"cell_type":"code","execution_count":null,"id":"Af_29guLcX7t","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1684134192545,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"Af_29guLcX7t","outputId":"d1517ff3-0385-4bc0-9473-1272c2f8cadb"},"outputs":[{"data":{"text/plain":["(220025, 2)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["labels_df.shape"]},{"cell_type":"code","execution_count":null,"id":"7CPmZzehcZM4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1684134192545,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"7CPmZzehcZM4","outputId":"a9e8c777-8e43-4824-9f30-7fcac8bc2acf"},"outputs":[{"data":{"text/plain":["(89117, 2)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["malignant_labels_df.shape"]},{"cell_type":"code","execution_count":null,"id":"OpQgTo_CccGV","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1684134192546,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"OpQgTo_CccGV","outputId":"492ebcb1-3841-4baa-cc9a-119cc8180e5f"},"outputs":[{"data":{"text/plain":["(130908, 2)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["non_malignant_labels_df.shape"]},{"cell_type":"code","execution_count":null,"id":"Dbxln8mlccbG","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1684134192546,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"Dbxln8mlccbG","outputId":"c30fd627-3c9c-4932-93c0-07bb602b349c"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-fcf9a012-eb5c-47dc-a273-d25589ab4bbd\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fcf9a012-eb5c-47dc-a273-d25589ab4bbd')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-fcf9a012-eb5c-47dc-a273-d25589ab4bbd button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-fcf9a012-eb5c-47dc-a273-d25589ab4bbd');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["Empty DataFrame\n","Columns: [id, label]\n","Index: []"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["labels_df[labels_df.duplicated(keep=False)]"]},{"cell_type":"code","execution_count":null,"id":"Yd7tV08oceAP","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1684134192546,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"Yd7tV08oceAP","outputId":"e1ae90bc-d341-441b-ac18-5fa2cf3a3492"},"outputs":[{"data":{"text/plain":["0    130908\n","1     89117\n","Name: label, dtype: int64"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["labels_df['label'].value_counts()"]},{"cell_type":"code","execution_count":null,"id":"WnK4jTY4ckqg","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1230,"status":"ok","timestamp":1684134193768,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"WnK4jTY4ckqg","outputId":"5c98748f-dce9-44e8-b2a4-83aa1eebbfd7"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/train\n","220025\n","/content\n"]}],"source":["%cd train\n","! ls | wc -l\n","%cd .."]},{"cell_type":"code","execution_count":null,"id":"5nC7PWmudA-t","metadata":{"id":"5nC7PWmudA-t"},"outputs":[],"source":["img_path = './train'\n","\n","malignant = labels_df.loc[labels_df['label'] == 1]['id'].values\n","non_malignant = labels_df.loc[labels_df['label'] == 0]['id'].values"]},{"cell_type":"code","execution_count":null,"id":"OQv2bqBLdCzG","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1684134193768,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"OQv2bqBLdCzG","outputId":"a121b32d-c271-42cf-b772-e994d90519a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["First 10 Non-Malignant IDs\n","========================================\n","f38a6374c348f90b587e046aac6079959adf3835\n","755db6279dae599ebb4d39a9123cce439965282d\n","bc3f0c64fb968ff4a8bd33af6971ecae77c75e08\n","068aba587a4950175d04c680d38943fd488d6a9d\n","acfe80838488fae3c89bd21ade75be5c34e66be7\n","559e55a64c9ba828f700e948f6886f4cea919261\n","8eaaa7a400aa79d36c2440a4aa101cc14256cda4\n","a106469bbfda4cdc5a9da7ac0152927bf1b4a92d\n","a1991e73a9b676faddd2bd47c39754b14d1eb923\n","08566ce82d4406f464c9c2a3cd014704735db7a9\n"]}],"source":["print('First 10 Non-Malignant IDs')\n","print('='*40)\n","for x in range(10): print(non_malignant[x])"]},{"cell_type":"code","execution_count":null,"id":"Cm6C0zjwdGF-","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1684134193769,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"Cm6C0zjwdGF-","outputId":"62277350-0e1b-47f1-97ac-c4ca1759aeaf"},"outputs":[{"name":"stdout","output_type":"stream","text":["First 10 Malignant IDs\n","========================================\n","c18f2d887b7ae4f6742ee445113fa1aef383ed77\n","a24ce148f6ffa7ef8eefb4efb12ebffe8dd700da\n","7f6ccae485af121e0b6ee733022e226ee6b0c65f\n","c3d660212bf2a11c994e0eadff13770a9927b731\n","94fa32b29cc1c00403176c0795fffa3cfaa0f20e\n","0b820b71670c039dd0a51333d1c919f471a9e940\n","d34af1e7500f2f3de41b0e6fdeb2ed245d814590\n","464327050ef07bb927f8bfb5c4e4dd5ebd4d3c09\n","6961bdcc16f6c1d7db88fc6a7823178288c2a29e\n","233bf46a575c1731821073e318c029e5df8b12ff\n"]}],"source":["print('First 10 Malignant IDs')\n","print('='*40)\n","for x in range(10): print(malignant[x])"]},{"cell_type":"code","execution_count":null,"id":"oIdHcgd9dH2R","metadata":{"id":"oIdHcgd9dH2R"},"outputs":[],"source":["def plot_fig(ids:str=None, title:str=None, nrows:int=5, ncols:int=15):\n","    \n","    fig, ax = plt.subplots(nrows, ncols, figsize=(18,6))\n","    plt.subplots_adjust(wspace=0, hspace=0)\n","\n","    for i,j in enumerate(ids[:nrows*ncols]):\n","        file_name = os.path.join(img_path, j + '.tif')\n","        img = Image.open(file_name)\n","\n","        id_col = ImageDraw.Draw(img)\n","        id_col.rectangle(((0,0), (95,95)), outline='white')\n","\n","        plt.subplot(nrows, ncols, i+1)\n","        plt.imshow(np.array(img))\n","        plt.axis('off')\n","\n","    plt.suptitle(title, y=0.92)"]},{"cell_type":"code","execution_count":null,"id":"VCnu3A54dKUp","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":489,"output_embedded_package_id":"18f9N0IeVO3xDhniQFk62cc8N_BIgtOrw"},"executionInfo":{"elapsed":14120,"status":"ok","timestamp":1684134207886,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"VCnu3A54dKUp","outputId":"1dc8cf50-8972-428c-9690-e033d5036687"},"outputs":[{"data":{"text/plain":["Output hidden; open in https://colab.research.google.com to view."]},"metadata":{},"output_type":"display_data"}],"source":["plot_fig(malignant, 'Malignant Cases')"]},{"cell_type":"code","execution_count":null,"id":"21eSMRjudKST","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":489,"output_embedded_package_id":"1Nj3ikZyL_nKGcGpBOjWyPx6bA8y9GUUN"},"executionInfo":{"elapsed":12237,"status":"ok","timestamp":1684134220112,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"21eSMRjudKST","outputId":"4f463513-fe6d-48e2-8326-06f3082ca125"},"outputs":[{"data":{"text/plain":["Output hidden; open in https://colab.research.google.com to view."]},"metadata":{},"output_type":"display_data"}],"source":["plot_fig(non_malignant, 'Non-Malignant Cases')"]},{"cell_type":"code","execution_count":null,"id":"BXJtDyWjdKPq","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1684134220113,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"BXJtDyWjdKPq","outputId":"46ae7f41-45c7-47cf-a066-5b39ff61f730"},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x7f72bbb9e050>"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["torch.manual_seed(12664675) # We create a manual seed"]},{"cell_type":"code","execution_count":null,"id":"NAN9PO55dKNk","metadata":{"id":"NAN9PO55dKNk"},"outputs":[],"source":["class HistoDataset(Dataset):\n","\n","    def __init__(self, data_dir, data_type='train', data_partition=None, split=None, seed=None, num_samples=None, **kwargs):\n","        ## Get image file names\n","        data_type_dir = os.path.join(data_dir, data_type) ## \"Data dir\" simply acts as a placeholder for the current directory, \"data_type\" defaults to \"train\", given it states the folder of avaialble data, and we do not have labels distincting the data in \"test\"\n","\n","        if 'is_malignant' in kwargs:\n","            self.is_malignant = kwargs['is_malignant']\n","        else:\n","            # Default self.is_malignant to True\n","            self.is_malignant = True\n","        \n","        ## Change the file names based on whether we are asking for a malignant or non-malignant dataset\n","        self.labels_df = malignant_labels_df['id'] if self.is_malignant else non_malignant_labels_df['id']\n","        self.file_names = self.labels_df.values  \n","\n","        ## If 'split' is provided split the data into train/test proportions\n","        if split is not None:\n","            assert 0.0 < split < 1.0, 'Split should be between 0.0 and 1.0'\n","            if seed is not None:\n","                np.random.seed(seed)\n","            np.random.shuffle(self.file_names)\n","            split_idx = int(len(self.file_names) * split)\n","            if data_partition == 'train':\n","                self.file_names = self.file_names[:split_idx]\n","            elif data_partition == 'eval':\n","                self.file_names = self.file_names[split_idx:]\n","            else:\n","                raise ValueError('\\\"data_partition\\\" invalid: should be \\\"train\\\" or \\\"test\\\"')\n","\n","        ## Selects number of samples based on kwargs for number of samples to select, raises error if number of samples is greater than total number of files\n","        if num_samples is not None:\n","            if num_samples == 'all':\n","                pass\n","            elif num_samples > len(self.file_names):\n","                raise ValueError('\\\"num_samples\\\" cannot be larger than the total number of samples')\n","            else:\n","                self.file_names = self.file_names[:num_samples]\n","\n","        ## Continuation of previous block, sets the number of samples as the length of file names (after being chosen by previous block)\n","        self.num_samples = len(self.file_names)\n","\n","        ## Sets the parameterized transformations available to creating the dataset\n","        if 'transforms' in kwargs:\n","            self.transforms = kwargs['transforms']\n","        else:\n","            self.transforms = transforms.Compose([])\n","\n","        ## Code block to return only the center crop 32x32 pixels that were used in the PCam dataset to identify whether to only use the cancer-identified pixels\n","        if 'malignant_crop' in kwargs:\n","            self.malignant_crop = kwargs['malignant_crop']\n","            self.return_malignant_section = True\n","            self.image_size = 32\n","        else:\n","            self.return_malignant_section = False\n","            self.image_size = 96\n","\n","        ## Using kwargs sets whether to return a PIL or torch.tensor representation of the image\n","        if 'return_PIL' in kwargs:\n","            self.return_PIL = kwargs['return_PIL']\n","        else:\n","            self.return_PIL = True\n","\n","        ## Sets number of channels in image, else defaults to 3 (R,G,B)\n","        if 'channels' in kwargs:\n","            self.channels = kwargs['channels']\n","        else:\n","            self.channels = 3\n","\n","        '''\n","        Defines the reverse transforms to be used later on, depending on the number of desired channels and\n","            whether or not a PIL Image is wanting to be returned\n","        '''\n","        if 'reverse_transforms_list' in kwargs:\n","            intermediate_transform = Lambda(lambda t: rearrange(t, 'i j -> j i')) if self.channels == 1 else Lambda(lambda t: rearrange(t, 'i j k -> j k i'))\n","            self.reverse_transforms_list = kwargs['reverse_transforms_list']\n","\n","            if 'reverse_return_PIL' in kwargs:\n","                self.reverse_return_PIL = kwargs['reverse_return_PIL']\n","\n","            for idx in range(len(self.reverse_transforms_list)):\n","                if self.reverse_transforms_list[idx] == 'channels':\n","                    self.reverse_transforms_list[idx] = intermediate_transform\n","\n","                elif self.reverse_transforms_list[idx] == 'reverse_return_PIL':\n","                    self.reverse_transforms_list[idx] = ToPILImage() if self.reverse_return_PIL else Compose([])\n","\n","            self.reverse_transforms = Compose(self.reverse_transforms_list)\n","        else:\n","            self.reverse_transforms = None\n","\n","        ## Choose indices for random samples, else use all samples if self.num_samples == len(self.file_names)\n","        self.files_choose = np.random.choice(self.file_names, \n","                                             self.num_samples, \n","                                             replace=False).tolist()\n","\n","        ## Get all file pathes that make up the selected images in the dataset\n","        self.full_file_path = [os.path.join(data_type_dir, file_name + '.tif') for file_name in self.files_choose]\n","\n","        ## Defines a pytorch.transforms functions to transform from PIL Image to PyTorch Tensor\n","        self.to_tensor = transforms.ToTensor()\n","\n","    def __len__(self):\n","        return self.num_samples\n","\n","    def __getitem__(self, idx):\n","        # Open image, apply transforms and return with label\n","        image = Image.open(self.full_file_path[idx])  # Open Image with PIL\n","        \n","        if self.channels == 1:\n","            image = image.convert('L')\n","\n","        if self.return_malignant_section:\n","            image = self.malignant_crop(image)\n","\n","        image_tensor = self.transforms(image) # Apply set transformations to Image\n","\n","        if self.return_PIL:\n","            return image\n","        else:\n","            return image_tensor"]},{"cell_type":"code","execution_count":null,"id":"fyT8PY6sdKKs","metadata":{"id":"fyT8PY6sdKKs"},"outputs":[],"source":["malignant_crop = transforms.CenterCrop((32, 32)) # We only want the innermost, identifiable 32x32px region of tumor tissue"]},{"cell_type":"code","execution_count":null,"id":"yYacD2-4dKH1","metadata":{"id":"yYacD2-4dKH1"},"outputs":[],"source":["## As stated in the DDPM paper, restricts the values of all pixels between [-1,1]\n","img_transforms = Compose(\n","    [\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Lambda(lambda t: (2 * t) - 1)\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"id":"4Uk18ELD9wP0","metadata":{"id":"4Uk18ELD9wP0"},"outputs":[],"source":["## numpy implementation for reverse_transforms, moving values: [-1, 1] -> [1, 0]\n","reverse_transforms_list_np = [\n","    Lambda(lambda t: t.transpose(1, 2, 0)),\n","    Lambda(lambda t: (t + 1) / 2),\n","    'channels', ## Will replace programatically based on whether using grayscale or RBG (1 or 3 channels)\n","    Lambda(lambda t: np.clip(t, 0, 1)),\n","    'reverse_return_PIL', ## Will replace programatically based on whether needing to return a PIL Image\n","]"]},{"cell_type":"code","execution_count":null,"id":"yrBGmZNcyKjH","metadata":{"id":"yrBGmZNcyKjH"},"outputs":[],"source":["## torch implementation for reverse_transforms, moving values: [-1, 1] -> [1, 0]\n","reverse_transforms_list = [\n","    Lambda(lambda t: t.permute(1, 2, 0)),\n","    Lambda(lambda t: (t + 1) / 2),\n","    'channels', ## Will replace programatically based on whether using grayscale or RBG (1 or 3 channels)\n","    Lambda(lambda t: torch.clamp(t, 0, 1)),\n","    'reverse_return_PIL', ## Will replace programatically based on whether needing to return a PIL Image\n","]"]},{"cell_type":"code","execution_count":null,"id":"3a2m-t307UZk","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1684134220114,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"3a2m-t307UZk","outputId":"6ef720f2-eaac-4f79-b244-0e359294f3c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n"]}],"source":["! pwd"]},{"cell_type":"code","execution_count":null,"id":"9ewRgMVl7Y2U","metadata":{"id":"9ewRgMVl7Y2U"},"outputs":[],"source":["data_dir = '.'"]},{"cell_type":"code","execution_count":null,"id":"Q7uK1XEldKCW","metadata":{"id":"Q7uK1XEldKCW"},"outputs":[],"source":["## Instantiate the dataset using malignant/non-malignant samples \n","train_dataset = HistoDataset(data_dir=data_dir,\n","                             data_type='train',\n","                             data_partition='train',\n","                             split=0.9, \n","                             seed=12664675, \n","                             num_samples='all',\n","                             is_malignant=False,\n","                             malignant_crop=malignant_crop,\n","                             transforms=img_transforms,\n","                             reverse_transforms_list=reverse_transforms_list,\n","                             reverse_return_PIL=False,\n","                             return_PIL=True,\n","                             channels=3\n",")"]},{"cell_type":"code","execution_count":null,"id":"glEQUGDM7wRF","metadata":{"id":"glEQUGDM7wRF"},"outputs":[],"source":["eval_dataset = HistoDataset(data_dir=data_dir, \n","                            data_type='train',\n","                            data_partition='eval',\n","                            split=0.9, \n","                            seed=12664675, \n","                            num_samples='all', \n","                            is_malignant=False,\n","                            malignant_crop=malignant_crop,\n","                            transforms=img_transforms,\n","                            reverse_transforms_list=reverse_transforms_list,\n","                            reverse_return_PIL=False,\n","                            return_PIL=True,\n","                            channels=3\n",")"]},{"cell_type":"markdown","id":"cc01c63b","metadata":{"id":"cc01c63b"},"source":["The `denoise_model` will be our U-Net defined above. We'll employ the Huber loss between the true and the predicted noise.\n","\n","## Define a PyTorch DataLoader\n","\n","Here we define a regular PyTorch DataLoader, as well as creating variables to store information about the dataset (`image_size`, `channels`, and `batch_size`)\n","\n","Finally, we define a `DataLoader` from the `HistoDataset` class created and instantiated previously"]},{"cell_type":"code","execution_count":null,"id":"6134d691","metadata":{"id":"6134d691"},"outputs":[],"source":["image_size = train_dataset.image_size\n","channels = train_dataset.channels\n","batch_size = 64"]},{"cell_type":"markdown","id":"db6f5875","metadata":{"id":"db6f5875"},"source":["Next, we define a function which we'll apply on-the-fly on the entire dataset. We use the `with_transform` [functionality](https://huggingface.co/docs/datasets/v2.2.1/en/package_reference/main_classes#datasets.Dataset.with_transform) for that. The function just applies some basic image preprocessing: random horizontal flips, rescaling and finally make them have values in the $[-1,1]$ range."]},{"cell_type":"code","execution_count":null,"id":"b3e78945","metadata":{"id":"b3e78945"},"outputs":[],"source":["## Changes the train/eval_dataset instance variable to return a torch.Tensor instead of a PILImage\n","train_dataset.return_PIL = False\n","eval_dataset.return_PIL = False\n","\n","## Create train/eval_loader\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"markdown","id":"6CtJltDJOqKg","metadata":{"id":"6CtJltDJOqKg"},"source":["# UNet Associated Classes\n","\n","---\n","\n","## Base Block"]},{"cell_type":"code","execution_count":null,"id":"63qmEqb_MVfI","metadata":{"id":"63qmEqb_MVfI"},"outputs":[],"source":["class BaseBlock(nn.Module):\n","    '''\n","    Base block for UNet with time embedding\n","    '''\n","    def __init__(self, in_ch, out_ch, t_dim=512):\n","        super().__init__()\n","        self.t_mlp = nn.Linear(t_dim, in_ch)\n","\n","        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(out_ch)\n","\n","        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(out_ch)\n","\n","        self.relu = nn.ReLU(inplace=True)\n","        \n","    def forward(self, x, t):\n","        t_emb = self.t_mlp(t)\n","        t_emb = t_emb.reshape(*t_emb.shape, 1, 1)\n","\n","        x = x + t_emb # time is added (what about concat?)\n","\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = self.relu(x)\n","\n","        return x"]},{"cell_type":"markdown","id":"RNzLGvHYOlln","metadata":{"id":"RNzLGvHYOlln"},"source":["## Encoder/Decoder Blocks"]},{"cell_type":"code","execution_count":null,"id":"uaQwrHiYMyCg","metadata":{"id":"uaQwrHiYMyCg"},"outputs":[],"source":["class EncoderBlock(nn.Module):\n","    '''\n","    UNet encoder\n","    '''\n","    def __init__(self, in_ch, out_ch):\n","        super().__init__()\n","        self.base = BaseBlock(in_ch, out_ch)\n","        self.downsample = nn.MaxPool2d(2)\n","        \n","    def forward(self, x, t):\n","        x = self.base(x, t)\n","        x_down = self.downsample(x)\n","\n","        return x_down, x # Return x for residual into Decoder\n","\n","class DecoderBlock(nn.Module):\n","    '''\n","    UNet decoder\n","    '''\n","    def __init__(self, in_ch, out_ch):\n","        super().__init__()\n","        self.upsample = nn.ConvTranspose2d(in_ch, out_ch, 2, 2)\n","        self.base = BaseBlock(in_ch, out_ch)\n","        \n","    def forward(self, x, f, t):\n","        x = self.upsample(x)\n","        x = torch.cat([x, f], dim=1) # Concat residual features from Encoder\n","        x = self.base(x, t)\n","\n","        return x"]},{"cell_type":"markdown","id":"Sdjlzb-fO2UX","metadata":{"id":"Sdjlzb-fO2UX"},"source":["## Sinusodial Positional Embedding with (Optional) Self-Attention"]},{"cell_type":"code","execution_count":null,"id":"5xxKxcgyMyu9","metadata":{"id":"5xxKxcgyMyu9"},"outputs":[],"source":["class SinusoidalPositionEmbedding(nn.Module):\n","    '''\n","    Transformer Sinusoidal Position(Time) Embedding in 'Attention is All You Need' paper\n","    '''\n","    def __init__(self, t_dim=512, device='cuda'):\n","        super().__init__()\n","        n = 10000\n","        self.w = 1. / torch.pow(n, 2 * torch.arange(t_dim // 2, device=device) / t_dim)\n","                \n","    def forward(self, t):\n","        t = t[:, None] * self.w[None, :]\n","        sin_t = torch.sin(t)\n","        cos_t = torch.cos(t)\n","        embedding = torch.stack([sin_t, cos_t], dim=-1).flatten(start_dim=1)\n","        return embedding\n","\n","class SelfAttention(nn.Module):\n","    def __init__(self, channels, heads=4):\n","        super(SelfAttention, self).__init__()\n","        self.channels = channels\n","        self.heads = heads\n","        self.mha = nn.MultiheadAttention(channels, heads, batch_first=True)\n","        self.layer_norm = nn.LayerNorm([channels])\n","        self.ff_self = nn.Sequential(\n","            nn.LayerNorm([channels]),\n","            nn.Linear(channels, channels),\n","            nn.GELU(),\n","            nn.Linear(channels, channels),\n","        )\n","\n","    def forward(self, x):\n","        batch_size, channels, height, width = x.shape\n","        x = x.view(batch_size, channels, -1).permute(0, 2, 1)\n","        x_ln = self.layer_norm(x)\n","\n","        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n","        attention_value = attention_value + x\n","        attention_value = self.ff_self(attention_value) + attention_value\n","        attention_value = attention_value.permute(0, 2, 1).view(batch_size, channels, height, width)\n","\n","        return attention_value"]},{"cell_type":"markdown","id":"YeDjFRDgO9nG","metadata":{"id":"YeDjFRDgO9nG"},"source":["## U-Net Class"]},{"cell_type":"code","execution_count":null,"id":"g0N5fzRLM4_U","metadata":{"id":"g0N5fzRLM4_U"},"outputs":[],"source":["class UNet(nn.Module):\n","    def __init__(self, attention_layers=(2, 4, 6, 8), t_dim=512):\n","        super().__init__()\n","\n","        # Time embedding\n","        self.t_mlp = nn.Sequential(\n","            SinusoidalPositionEmbedding(),\n","            nn.Linear(t_dim, t_dim),\n","        )\n","\n","        # Encoder blocks\n","        self.enc_1 = EncoderBlock(3, 64)\n","        self.sa_enc_1 = SelfAttention(64) if (1 in attention_layers) else nn.Identity()\n","\n","        self.enc_2 = EncoderBlock(64, 128)\n","        self.sa_enc_2 = SelfAttention(128) if (2 in attention_layers) else nn.Identity()\n","\n","        self.enc_3 = EncoderBlock(128, 256)\n","        self.sa_enc_3 = SelfAttention(256) if (3 in attention_layers) else nn.Identity()\n","\n","        self.enc_4 = EncoderBlock(256, 512)\n","        self.sa_enc_4 = SelfAttention(512) if (4 in attention_layers) else nn.Identity()\n","\n","        # Mid block\n","        self.mid_5 = BaseBlock(512, 1024)\n","        self.sa_mid_5 = SelfAttention(1024) if (5 in attention_layers) else nn.Identity()\n","\n","        # Decoder blocks\n","        self.dec_6 = DecoderBlock(1024, 512)\n","        self.sa_dec_6 = SelfAttention(512) if (6 in attention_layers) else nn.Identity()\n","\n","        self.dec_7 = DecoderBlock(512, 256)\n","        self.sa_dec_7 = SelfAttention(256) if (7 in attention_layers) else nn.Identity()\n","\n","        self.dec_8 = DecoderBlock(256, 128)\n","        self.sa_dec_8 = SelfAttention(128) if (8 in attention_layers) else nn.Identity()\n","\n","        self.dec_9 = DecoderBlock(128, 64)\n","        self.sa_dec_9 = SelfAttention(64) if (9 in attention_layers) else nn.Identity()\n","\n","        # Final decode\n","        self.out = nn.Conv2d(64, 3, 1)\n","        \n","    def forward(self, x, t):\n","        t = self.t_mlp(t)\n","        \n","        x, r_1 = self.enc_1(x, t)\n","        x = self.sa_enc_1(x)\n","\n","        x, r_2 = self.enc_2(x, t)\n","        x = self.sa_enc_2(x)\n","\n","        x, r_3 = self.enc_3(x, t)\n","        x = self.sa_enc_3(x)\n","\n","        x, r_4 = self.enc_4(x, t)\n","        x = self.sa_enc_4(x)\n","\n","        x = self.mid_5(x, t)\n","        x = self.sa_mid_5(x)\n","\n","        x = self.dec_6(x, r_4, t)\n","        x = self.sa_dec_6(x)\n","\n","        x = self.dec_7(x, r_3, t)\n","        x = self.sa_dec_7(x)\n","\n","        x = self.dec_8(x, r_2, t)\n","        x = self.sa_dec_8(x)\n","\n","        x = self.dec_9(x, r_1, t)\n","        x = self.sa_dec_9(x)\n","        \n","        x = self.out(x)\n","\n","        return x"]},{"cell_type":"markdown","id":"yF9fsEVKcRXU","metadata":{"id":"yF9fsEVKcRXU"},"source":["# Diffusion Class\n","\n","---"]},{"cell_type":"code","execution_count":null,"id":"8tKX9GYfVoxH","metadata":{"id":"8tKX9GYfVoxH"},"outputs":[],"source":["class DiffusionModel():\n","    def __init__(self, channels=3, timesteps=1000, schedule='cosine_beta_schedule', loss_type='huber', attention_layers=(2, 4, 6, 8), beta_start=1e-4, beta_end=2e-2, s=0.008, image_size=32, device='cuda'):\n","        \n","        ## Instantiation attributes/objects from parameters\n","        self.timesteps = timesteps\n","        \n","        self.schedule = schedule\n","        self.attention_layers = attention_layers\n","\n","        self.beta_start = beta_start\n","        self.beta_end = beta_end\n","        self.s = s\n","\n","        self.channels = channels\n","        self.image_size = image_size\n","        self.device = device\n","\n","        ## Obtain loss function based on what loss type is wanted\n","        self.loss_fn = {'huber': F.smooth_l1_loss, 'l1': F.l1_loss, 'l2': F.mse_loss}.get(loss_type)\n","\n","\n","        ## Derived attributes/objects\n","        self.beta = self.cosine_beta_schedule().to(self.device) if self.schedule == 'cosine_beta_schedule' else \\\n","                    self.linear_beta_schedule().to(self.device)\n","        \n","        self.alpha = (1. - self.beta).to(self.device)\n","        self.alpha_bar = torch.cumprod(self.alpha, dim=0).to(self.device)\n","\n","        self.unet = UNet(channels=self.channels, attention_layers=self.attention_layers)\n","\n","    def cosine_beta_schedule(self):\n","        '''\n","        cosine beta schedule as proposed in https://arxiv.org/abs/2102.09672\n","        '''\n","        total_steps = self.timesteps + 1\n","        x = torch.linspace(0, self.timesteps, total_steps)\n","        alpha_bar = torch.pow(torch.cos(((x / self.timesteps) + self.s) / (1 + self.s) * torch.pi * 0.5), 2)\n","        alpha_bar = alpha_bar / alpha_bar[0]\n","        beta = 1 - (alpha_bar[1:] / alpha_bar[:-1])\n","        return torch.clip(beta, 0.0001, 0.9999)\n","\n","    def linear_beta_schedule(self):\n","        return torch.linspace(self.beta_start, self.beta_end, self.timesteps)\n","\n","    def forward_process(self, x_0, t):\n","        '''\n","        aka diffusion process\n","        '''\n","        eps = torch.randn_like(x_0, device=self.device)\n","        alpha_bar_t = self.alpha_bar[t].reshape(-1, *((1,) * (len(x_0.shape) - 1)))\n","        x_t = torch.sqrt(alpha_bar_t) * x_0 + torch.sqrt(1 - alpha_bar_t) * eps ## Reparameterization trick\n","        return x_t, eps\n","\n","    def loss_function(self, x_0, t):\n","        '''\n","        simple MSE loss in DDPM paper\n","        '''\n","        x_t, eps = self.forward_process(x_0, t) ## t used as index\n","        eps_pred = self.unet(x_t, t.clone().to(dtype=torch.float32))\n","        loss = self.loss_fn(eps, eps_pred) ## huber loss\n","        return loss\n","\n","    @staticmethod\n","    def reverse_transform(img):\n","        '''\n","        Returns an image transformed from\n","            [-1, 1] -> [0, 1] ([format in paper] -> [PIL image format])\n","            Automatically determines whether image is a torch.Tensor object and transforms based on type\n","        @param img: A torch.Tensor or numpy.ndarray image with values between [-1, 1]\n","        @return img: A numpy.ndarray image with values between [0, 1]\n","        '''\n","        if isinstance(img, torch.Tensor):\n","            img = img.detach().cpu().numpy()\n","\n","        img = img.transpose(1, 2, 0)\n","        img = (img + 1) / 2\n","        img = np.clip(img, 0, 1)\n","\n","        return img\n","\n","    def show_images(self, imgs, reverse_transform=True):\n","        '''\n","        Plots outputs in format of generated images in a list or batches of images in numpy.array or torch.tensor format\n","            @param imgs: list, numpy.array, or torch.tensor of images\n","            @param reverse_transform: Reverses the transformed values of [-1, 1] back to [0, 1], as accepted by imshow and other plotting packages\n","        '''\n","        nums = len(imgs) if isinstance(imgs, list) else 1 if len(imgs.shape) == 3 else imgs.shape[0] # list or batch (of numpy or torch) of single or multiple images\n","        num_rows = (nums + 9) // 10\n","        num_cols = min(nums, 10)\n","\n","        fig = plt.figure(figsize=(num_cols, num_rows))\n","        for idx in range(nums):\n","            img = imgs[idx] if nums > 1 else imgs\n","\n","            if reverse_transform:\n","                img = self.reverse_transform(img)\n","\n","            ax = plt.subplot(num_rows, num_cols, idx + 1)\n","            plt.imshow(img)\n","            plt.axis('off')\n","        plt.subplots_adjust(wspace=0.1, hspace=0.1)\n","        plt.show()\n","\n","    @torch.no_grad()\n","    def sample_image(self, device='cuda'):\n","        '''\n","        Implementation of Algorithm 2 Sampling, for the same model across a single batch, across all timesteps\n","            @param device: Specifying which device to run processes on, defaults to 'cuda'\n","            @return x_p: For a single image processed, a torch.tensor of shape (timesteps x channels x height x width)\n","            @return x_img: Returns the last timestep of x_p, or a torch.tensor of shape (channels x height x width)\n","        '''\n","        model = self.unet\n","        model.eval()\n","\n","        x = torch.randn((1, self.channels, self.image_size, self.image_size), device=device)\n","        x_p = torch.empty((self.timesteps, self.channels, self.image_size, self.image_size))\n","\n","        for t in tqdm(reversed(range(0, self.timesteps)), desc='Sampling timestep loop', total=self.timesteps):\n","            z = torch.randn_like(x) if t > 1 else torch.zeros_like(x)\n","            z = z.to(device)\n","\n","            t_tensor = torch.tensor(t, dtype=torch.float32).unsqueeze(0).to(device)\n","            eps_pred = self.unet(x, t_tensor)\n","\n","            x_pred = 1 / torch.sqrt(self.alpha[t]) * (x - self.beta[t] / torch.sqrt(1 - self.alpha_bar[t]) * eps_pred) + torch.sqrt(self.beta[t]) * z\n","            x = x_pred ## p(x_{t-1}|x_t)\n","            x_p[-t] = x.cpu()\n","            x_img = x_p[-1]\n","\n","        return x_p, x_img\n","\n","    @torch.no_grad()\n","    def sample_batch(self, batch_size=32, device='cuda', return_torch=False):\n","        '''\n","        Implementation of batched version for Algorithm 2 Sampling for the same model, across all batches.\n","            @param batch_size: Taking an int specifying the size per batch to process\n","            @param device: Specifying which device to run processes on, defaults to 'cuda'\n","            @return x_b: Batched torch.Tensor version of images processed across ALL timesteps\n","            @return x_b.numpy(): Batched numpy array version of images processed across ALL timesteps\n","            @return x_b_img: Batched torch.tensor single image\n","            @return x_b_img.numpy(): Batched np.array single image\n","        '''\n","\n","        x_b = torch.empty((batch_size, self.timesteps, self.channels, self.image_size, self.image_size))\n","        x_b_img = torch.empty((batch_size, self.channels, self.image_size, self.image_size))\n","        for idx in tqdm(range(batch_size), desc='Sampling batch loop', total=batch_size):\n","            x_p, x_img = self.sample_image()\n","\n","            x_b[idx] = x_p\n","            x_b_img[idx] = x_img\n","\n","        if return_torch:\n","            return x_b, x_b_img\n","        else:\n","            return x_b.numpy(), x_b_img.numpy()"]},{"cell_type":"markdown","id":"1nskiuzeGdP8","metadata":{"id":"1nskiuzeGdP8"},"source":["## Train the model\n","\n","Next, we train the model in regular PyTorch fashion.\n","\n","Below, we define the model, and move it to the GPU. We also define a standard optimizer (Adam).\n","\n","The environment variables\n","\n","```unix\n","IMPORT_MODEL\n","UPLOAD_MODEL\n","TRAIN_MODEL\n","SAMPLE_MODEL\n","```\n","\n","Set whether to acquire an already pre-trained model to further train or instantiate a new instance of the denoising `UNet` model and train from scratch"]},{"cell_type":"code","execution_count":null,"id":"m00CbLWYAfWe","metadata":{"id":"m00CbLWYAfWe"},"outputs":[],"source":["IMPORT_MODEL = False\n","UPLOAD_MODEL = False\n","TRAIN_MODEL = True\n","SAMPLE_MODEL = True"]},{"cell_type":"code","execution_count":null,"id":"TzazK2c6GkT2","metadata":{"id":"TzazK2c6GkT2"},"outputs":[],"source":["is_cancerous_str = \"cancerous\" if train_dataset.is_malignant else \"non_cancerous\""]},{"cell_type":"code","execution_count":null,"id":"tMGj0VBwGwM2","metadata":{"id":"tMGj0VBwGwM2"},"outputs":[],"source":["def get_max_epoch_available():\n","    MODEL_PATH = f'./drive/MyDrive/cv-final-project/models/{is_cancerous_str}/*.pt'\n","    LOSS_LIST_PATH = f'./drive/MyDrive/cv-final-project/loss_lists/{is_cancerous_str}/*.csv'\n","\n","    last_model_str = glob.glob(MODEL_PATH)[-1]\n","    last_model_int = int(re.search(r'(\\d+)(?=epoch)', last_model_str).group()) ## Gets the last available epoch in saved models\n","    \n","    last_loss_list_str = glob.glob(LOSS_LIST_PATH)[-1]\n","    last_loss_list_int = int(re.search(r'(\\d+)(?=epoch)', last_loss_list_str).group()) ## Gets the last available epoch in saved loss lists\n","\n","    if last_model_int != last_loss_list_int: ## Defaults to the largest of the same loss list or model, given there is a mismatch of final number of epoch\n","        min_epoch = min(last_model_int, last_loss_list_int)\n","        print(f'Final model does not equal final loss list, defaulting to smaller of two: {min_epoch}')\n","    else:\n","        min_epoch = last_model_int\n","\n","    return min_epoch"]},{"cell_type":"code","execution_count":null,"id":"naNS9NKLGyky","metadata":{"id":"naNS9NKLGyky"},"outputs":[],"source":["if IMPORT_MODEL: ## Imports model based on the flag IMPORT_MODEL\n","    if UPLOAD_MODEL:\n","        model = files.upload() ## Uses Google Colab to upload mdoel\n","        loss_list = files.upload()\n","    else:\n","        num_epoch_get = input('Enter the number of epochs model/loss_list to import: ') ## Uses model saved in Google Drive for a certain epoch\n","\n","        if num_epoch_get == 'max': ## If entry is 'max', will call the previous function to get the maximum available epoch'd model\n","            num_epoch_get = get_max_epoch_available()\n","\n","        PATH_TO_MODEL = f'./drive/MyDrive/cv-final-project/models/{is_cancerous_str}/RGB_model_{train_dataset.image_size}px_{str(num_epoch_get)}epoch_{is_cancerous_str}.pt'\n","        PATH_TO_LOSS_LIST = f'./drive/MyDrive/cv-final-project/loss_lists/{is_cancerous_str}/RGB_model_{train_dataset.image_size}px_{str(num_epoch_get)}epoch_{is_cancerous_str}_loss_list.csv'\n","        \n","        model = torch.load(PATH_TO_MODEL)\n","        loss_df = pd.read_csv(PATH_TO_LOSS_LIST)\n","        loss_list = list(loss_df['loss'])\n","\n","else: ## If not needing to import or upload a model (i.e. needing to create a new model), the below instantiates a new model and loss list\n","    diffusion = DiffusionModel(attention_layers=())\n","    loss_list = []"]},{"cell_type":"code","execution_count":null,"id":"S9Bjf8XnBeMO","metadata":{"id":"S9Bjf8XnBeMO"},"outputs":[],"source":["def save_diffusion(diffusion_model:DiffusionModel, save_str:str, save_to_drive:bool, schedule, attention):\n","\n","    if not save_to_drive:\n","        models_folder = Path('./models')\n","        models_folder.mkdir(exist_ok=True)\n","\n","        model_save_path = str(models_folder / save_str) + '.pt'\n","\n","    else:\n","        model_save_path = f'./drive/MyDrive/cv-final-project/models/{is_cancerous_str}/' + save_str + '.pt'\n","\n","    torch.save(diffusion_model, model_save_path)"]},{"cell_type":"code","execution_count":null,"id":"f-m-tvGjnetM","metadata":{"id":"f-m-tvGjnetM"},"outputs":[],"source":["def save_loss(loss_list, save_str:str, save_to_drive:bool, schedule, attention):\n","    \n","    if not save_to_drive:\n","        loss_list_folder = Path('./loss_list')\n","        loss_list_folder.mkdir(exist_ok=True)\n","\n","        loss_list_save_path = str(loss_list_folder / save_str) + '.csv'\n","\n","    else:\n","        loss_list_save_path = f'./drive/MyDrive/cv-final-project/loss_lists/{is_cancerous_str}/' + save_str + '_loss_list.csv'\n","\n","    loss_df = pd.DataFrame(loss_list)\n","    loss_df.to_csv(loss_list_save_path, header=False)"]},{"cell_type":"code","execution_count":null,"id":"yBR2MwmKyq_p","metadata":{"id":"yBR2MwmKyq_p"},"outputs":[],"source":["def train_model(diffusion,\n","                epochs:int=200,\n","                epoch_start=0,\n","                print_every_n_steps:int=100,\n","                save_diffusion_every_n_epochs:int=10,\n","                save_loss_every_n_epochs:int=1\n","):\n","\n","    ## Instantiate the diffusion class/model for whether to use any self-attention layers\n","    model = diffusion.get_unet()\n","    model.train()\n","\n","    ## Obtains model attention and schedule\n","    schedule = diffusion.schedule\n","    attention = 'no_attention' if len(diffusion.attention_layers) == 0 else 'self_attention'\n","\n","    ## Instantiates device, moves model to device. and sets optimizer to Adam\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    model.to(device)\n","    optimizer = Adam(model.parameters(), lr=1e-5)\n","\n","    model_channel_str = 'RGB_model_' if diffusion.channels == 3 else 'gray_model_'\n","\n","    loss_list = []\n","    model.train()\n","    for epoch in range(epoch_start+1, epoch_start+epochs+1):\n","        for step, batch in enumerate(train_loader, 1):\n","            optimizer.zero_grad() ## Zeroes out optimizer\n","\n","            batch_size = batch.shape[0] ## Defines batch_size as the first index of the batch.shape\n","            batch = batch.to(device) ## Sends batch to device\n","\n","            t = torch.randint(0, diffusion.timesteps, (batch_size,), device=device).long() ## Algorithm 1 line 3: sample t uniformally for every example in the batch\n","\n","            loss = diffusion.loss_function(model, batch, t)\n","            loss_value = loss.item() / len(train_loader)\n","\n","            if step % print_every_n_steps == 0 or step == 1: ## Prints out the first loss and every loss for previously determined step size\n","                print(f'Epoch: {epoch} | Step: {step} | Image: {step * batch_size}/{len(train_dataset)} | Model Type: [\\\"{schedule}\\\"; \\\"{attention}\\\"] | Loss: {loss_value}')\n","\n","            if epoch % save_diffusion_every_n_epochs == 0 and step == 1: ## Saves the diffusion model every n epochs, while also fulfilling that the number of steps be 0\n","                save_str = model_channel_str + f'{diffusion.image_size}px_{epoch}epoch_{is_cancerous_str}'\n","                save_diffusion(diffusion_model=diffusion, save_str=save_str, save_to_drive=True, schedule=schedule, attention=attention)\n","\n","            if step == 1: ## Adds to the loss list (used for graphing later on) at the beginning of every step\n","                loss_list.append(loss_value)\n","\n","                if epoch % save_loss_every_n_epochs == 0: ## Saves the loss list as a CSV file every n epochs\n","                    save_str = model_channel_str + f'{diffusion.image_size}px_{epoch}epoch_{is_cancerous_str}'\n","                    save_loss(loss_list=loss_list, save_str=save_str, save_to_drive=True, schedule=schedule, attention=attention)\n","\n","            loss.backward()\n","            optimizer.step()"]},{"cell_type":"code","execution_count":null,"id":"9SYN6W6MGrKL","metadata":{"id":"9SYN6W6MGrKL"},"outputs":[],"source":["epochs = 100\n","epoch_start = len(loss_list)\n","\n","print_every_n_steps = 100\n","save_diffusion_every_n_epochs = 10\n","save_loss_every_n_epochs = 10\n","\n","model_channel_str = 'RGB_model_' if train_dataset.channels == 3 else 'gray_model_'"]},{"cell_type":"code","execution_count":null,"id":"R8fjJRFeKgAB","metadata":{"id":"R8fjJRFeKgAB"},"outputs":[],"source":["train_model(epochs=epochs,\n","            epoch_start=epoch_start,\n","            print_every_n_steps=print_every_n_steps,\n","            save_diffusion_every_n_epochs=save_diffusion_every_n_epochs,\n","            save_loss_every_n_epochs=save_loss_every_n_epochs,\n","            diffusion=diffusion)"]},{"cell_type":"markdown","id":"a8337c82","metadata":{"id":"a8337c82"},"source":["## Sampling (Inference)\n","\n","To sample from the model, we can just use our sample function defined above:\n"]},{"cell_type":"code","execution_count":null,"id":"XAjv_EuvFIto","metadata":{"id":"XAjv_EuvFIto"},"outputs":[],"source":["if SAMPLE_MODEL:\n","    sample_batch_size = 128\n","    samples, samples_img = diffusion.sample_batch(diffusion, batch_size=sample_batch_size, return_torch=False)"]},{"cell_type":"code","execution_count":null,"id":"6hnq1iboKHEW","metadata":{"id":"6hnq1iboKHEW"},"outputs":[],"source":["diffusion.show_images(samples_img, reverse_process=True) if SAMPLE_MODEL else None"]},{"cell_type":"code","execution_count":null,"id":"_s-Al2lJ2c8T","metadata":{"id":"_s-Al2lJ2c8T"},"outputs":[],"source":["batch_idx = 43\n","diffusion.show_images(samples_img[batch_idx]) if SAMPLE_MODEL else None"]},{"cell_type":"markdown","id":"0k4H1fmlKvzR","metadata":{"id":"0k4H1fmlKvzR"},"source":["We can also create a gif of the denoising process using all saved timesteps within the timestep dimension of the reverse process:"]},{"cell_type":"code","execution_count":null,"id":"l8vvj9xluTSv","metadata":{"id":"l8vvj9xluTSv"},"outputs":[],"source":["reversed_samples = np.array([[diffusion.reverse_transform(timestep_image) for timestep_image in sample] for sample in samples]) if SAMPLE_MODEL else None"]},{"cell_type":"code","execution_count":null,"id":"spE1I9aVNwzZ","metadata":{"id":"spE1I9aVNwzZ"},"outputs":[],"source":["if SAMPLE_MODEL:\n","    fig, ax = plt.subplots()\n","    images = []\n","    for t in range(diffusion.timesteps):\n","        image = plt.imshow(reversed_samples[batch_idx][t], cmap='gray' if eval_dataset.channels == 1 else None, animated=True)\n","        images.append([image])\n","\n","    animate = animation.ArtistAnimation(fig, images, interval=10, repeat_delay=1000, blit=True)\n","    animate.save('diffusion.gif')"]},{"cell_type":"code","execution_count":null,"id":"GxhoKhAnc0Q9","metadata":{"id":"GxhoKhAnc0Q9"},"outputs":[],"source":["if SAMPLE_MODEL:    \n","    video = animate.to_html5_video()\n","    HTML(video)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1U5Hxq8dPG55969J6ZuUAr2wx7HPmuEcr","timestamp":1679821339964},{"file_id":"1GYp9jxDwiPu3vfU6-EQnQT5CuNQ-YAqP","timestamp":1679054123992},{"file_id":"https://github.com/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb","timestamp":1677736406553}]},"gpuClass":"standard","jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}
